import os
import torch
import random
import yaml
import argparse
from torch import nn
from torch.utils.data import DataLoader
from torchvision import transforms
from diffusers import AutoencoderKL
from tqdm import tqdm
import torch.optim.lr_scheduler as lr_scheduler
from torch.amp import GradScaler, autocast
import matplotlib.pyplot as plt


from dataset.dataset import EmojiDataset
from models.clip import CLIPTextEncoder
from models.diffusion import Diffusion
from schedulers.ddpm import DDPMScheduler
from utils.helpers import set_seed, count_parameters, plot_loss
from utils.time_embedding import embed_timesteps


def load_config(config_path):
    """Loads configuration from a YAML file."""
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        print(f"Error: Configuration file not found at {config_path}")
        exit(1)
    except yaml.YAMLError as e:
        print(f"Error parsing YAML file {config_path}: {e}")
        exit(1)


def setup_paths(args):
    """Creates output directories and generates full paths."""
    # Construct paths relative to the *script's location* or project root
    # Assuming train.py is in src/, and config paths are relative to project root
    project_root = os.path.dirname(
        os.path.dirname(__file__))  # Get parent dir of src/

    # Use output_dir from args (which defaults to YAML value if not overridden)
    base_output_dir = os.path.join(project_root, args.output_dir)

    model_save_dir = os.path.join(base_output_dir, args.model_subdir)
    plot_save_dir = os.path.join(base_output_dir, args.plot_subdir)

    os.makedirs(model_save_dir, exist_ok=True)
    os.makedirs(plot_save_dir, exist_ok=True)

    model_save_path = os.path.join(model_save_dir, args.model_filename)
    loss_plot_save_path = os.path.join(plot_save_dir, args.loss_plot_filename)

    # --- Data Paths ---
    # Assume data paths in YAML are relative to project root
    data_dir = os.path.join(project_root, args.data_dir)
    image_folder = os.path.join(data_dir, args.image_folder)
    csv_files = [os.path.join(data_dir, f) for f in args.csv_files]

    return model_save_dir, model_save_path, loss_plot_save_path, image_folder, csv_files


def main(args):
    # --- 1. Setup & Configuration ---
    set_seed(args.seed)
    device = torch.device(args.device if torch.cuda.is_available()
                          and args.device == "cuda" else "cpu")
    print(f"Using device: {device}")

    # Setup paths based on args
    model_save_dir, model_save_path, loss_plot_save_path, image_folder, csv_files = setup_paths(
        args)

    print(f"Configuration loaded from: {args.config}")
    print(
        f"Effective Config: Epochs={args.num_epochs}, BatchSize={args.batch_size}, LR={args.learning_rate}")
    # Print base output dir
    print(f"Outputs will be saved to: {os.path.dirname(model_save_dir)}")

    # --- 2. Load Models ---
    print("Loading VAE...")
    vae = AutoencoderKL.from_pretrained(args.vae_id).to(device)
    vae.requires_grad_(False)
    vae.eval()

    print("Loading CLIP Text Encoder...")
    text_encoder = CLIPTextEncoder().to(device)  # Uses clip_id from args

    print("Initializing Diffusion Model...")
    diffusion_model = Diffusion(
        h_dim=args.diffusion_hidden_dim,
        n_head=args.diffusion_num_heads
    ).to(device)

    # --- 3. Prepare Data ---
    print("Setting up Dataset and DataLoader...")
    transform = transforms.Compose([
        transforms.Resize(
            (args.image_width, args.image_height),
            interpolation=transforms.InterpolationMode.BICUBIC
        ),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[
                             0.5, 0.5, 0.5])  # Normalize to [-1, 1]
    ])

    train_dataset = EmojiDataset(
        csv_files=csv_files,  # Use path list generated by setup_paths
        image_folder=image_folder,  # Use path generated by setup_paths
        transform=transform
    )
    # Adjust pin_memory based on actual device being used
    pin_memory_flag = args.pin_memory if device.type == "cuda" else False
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=pin_memory_flag,
        # persistent_workers=True if args.num_workers > 0 else False
    )
    print(f"Dataset size: {len(train_dataset)}")

    # --- 4. Training Setup ---
    print("Configuring Optimizer, Scheduler, Loss, etc...")
    optimizer = torch.optim.AdamW(
        diffusion_model.parameters(),
        lr=args.learning_rate,
        weight_decay=args.optimizer_weight_decay
    )
    criterion = nn.MSELoss()

    noise_scheduler = DDPMScheduler(
        random_generator=None,
        train_timesteps=args.train_timesteps,
        diffusion_beta_start=args.beta_start,
        diffusion_beta_end=args.beta_end
    )

    # Configure LR Scheduler based on type
    if args.lr_scheduler_type == "cosine":
        num_training_steps = args.num_epochs * len(train_dataloader)
        t_max_steps = int(num_training_steps *
                          args.lr_scheduler_t_max_epochs_factor)
        # Note: CosineAnnealingLR usually uses T_max in epochs, but steps might be better if dataset size varies
        # Using epochs here for simplicity as per original notebook
        scheduler = lr_scheduler.CosineAnnealingLR(
            optimizer,
            # T_max in epochs
            T_max=int(args.num_epochs * args.lr_scheduler_t_max_epochs_factor),
            eta_min=args.lr_scheduler_eta_min
        )
        print(
            f"Using Cosine LR scheduler with T_max={scheduler.T_max} epochs, eta_min={args.lr_scheduler_eta_min}")
    # Add elif for other scheduler types ("linear", "constant") if needed
    else:
        scheduler = lr_scheduler.ConstantLR(
            optimizer, factor=1.0)  # Default to no decay
        print("Using Constant LR scheduler (no decay)")

    scaler = GradScaler(enabled=args.use_amp)

    # Print parameter counts
    print("-" * 30)
    print(f"VAE parameters (frozen): {count_parameters(vae)}")
    # text_encoder freezes internally
    print(
        f"CLIP Text Encoder parameters (frozen): {count_parameters(text_encoder)}")
    print(
        f"Diffusion Model parameters (trainable): {count_parameters(diffusion_model)}")
    print(f"Total Trainable Parameters: {count_parameters(diffusion_model)}")
    print("-" * 30)

    # --- 5. Training Loop ---
    print("Starting Training...")
    losses = []
    global_step = 0

    for epoch in range(args.num_epochs):
        diffusion_model.train()
        epoch_loss = 0.0
        progress_bar = tqdm(
            train_dataloader, desc=f"Epoch {epoch+1}/{args.num_epochs}", leave=False)

        for batch_idx, (images, titles) in enumerate(progress_bar):
            images = images.to(device)

            # Prepare prompts for classifier-free guidance
            processed_titles = []
            for title in titles:
                if random.random() < args.empty_prompt_prob:
                    processed_titles.append("")  # Empty prompt
                else:
                    clean_title = title.replace('"', '').replace("'", '')
                    processed_titles.append(f"A photo of {clean_title}")

            # --- Forward Pass ---
            with torch.no_grad():
                latents = vae.encode(
                    images).latent_dist.sample() * args.vae_scale_factor
                context_embeddings = text_encoder(processed_titles)

            timesteps = torch.randint(
                0, noise_scheduler.total_train_timesteps,
                (latents.shape[0],), device=device
            )

            noisy_latents, noise = noise_scheduler.add_noise(
                latents, timesteps)
            time_embeddings = embed_timesteps(
                timesteps, args.diffusion_time_emb_dim).to(device)

            with autocast(device_type=device.type, dtype=torch.float16, enabled=args.use_amp):
                noise_pred = diffusion_model(
                    noisy_latents, context_embeddings, time_embeddings)
                loss = criterion(noise_pred, noise)

            # --- Backward Pass & Optimization ---
            optimizer.zero_grad()
            scaler.scale(loss).backward()

            if args.grad_clip_max_norm is not None and args.grad_clip_max_norm > 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(
                    diffusion_model.parameters(), args.grad_clip_max_norm)

            scaler.step(optimizer)
            scaler.update()

            batch_loss = loss.item()
            epoch_loss += batch_loss
            global_step += 1

            # Update progress bar with loss and LR
            log_dict = {"loss": f"{batch_loss:.5f}",
                        "lr": f"{optimizer.param_groups[0]['lr']:.6f}"}
            progress_bar.set_postfix(log_dict)

            # Optional detailed logging per step
            # if args.log_freq and global_step % args.log_freq == 0:
            #     print(f" Step: {global_step}, Batch Loss: {batch_loss:.5f}")

        # --- End of Epoch ---
        lr_scheduler.step()  # Step the LR scheduler (after optimizer.step())
        avg_epoch_loss = epoch_loss / len(train_dataloader)
        losses.append(avg_epoch_loss)
        print(
            f"Epoch [{epoch+1}/{args.num_epochs}] - Avg Loss: {avg_epoch_loss:.5f} - LR: {optimizer.param_groups[0]['lr']:.6f}")

        # Save model periodically
        if (epoch + 1) % args.save_epoch_freq == 0 or (epoch + 1) == args.num_epochs:
            epoch_save_filename = args.model_filename.replace(
                ".pth", f"_epoch{epoch+1}.pth")
            current_save_path = os.path.join(
                model_save_dir, epoch_save_filename)
            print(f"Saving model checkpoint at epoch {epoch+1}...")
            torch.save(diffusion_model.state_dict(), current_save_path)
            print(f"Model saved to {current_save_path}")

    # --- Training Finished ---
    print("Training finished!")

    # Save the final model (optional, could just rely on last epoch save)
    # print("Saving final model...")
    # torch.save(diffusion_model.state_dict(), model_save_path)
    # print(f"Final model saved to {model_save_path}")

    # Plot and save the loss curve
    plot_loss(losses, loss_plot_save_path)


if __name__ == "__main__":
    # --- Argument Parsing ---
    parser = argparse.ArgumentParser(
        description="Train Diffusion Model for Emoji Generation using YAML config")
    parser.add_argument("--config", type=str, default="../config.yaml",  # Default config path relative to src/
                        help="Path to the YAML configuration file")

    # Load default config to set argparse defaults
    # Parse only the --config argument first
    temp_args, _ = parser.parse_known_args()
    config = load_config(temp_args.config)

    # Add arguments, using loaded YAML values as defaults
    # --- Data ---
    parser.add_argument("--data-dir", type=str,
                        default=config['data']['data_dir'])
    parser.add_argument("--image-folder", type=str,
                        default=config['data']['image_folder'])
    parser.add_argument("--csv-files", nargs='+',
                        default=config['data']['csv_files'])  # Expect list
    parser.add_argument("--image-width", type=int,
                        default=config['data']['image_width'])
    parser.add_argument("--image-height", type=int,
                        default=config['data']['image_height'])

    # --- Model ---
    parser.add_argument("--vae-id", type=str,
                        default=config['model']['vae_id'])
    parser.add_argument("--clip-id", type=str,
                        default=config['model']['clip_id'])
    parser.add_argument("--vae-scale-factor", type=float,
                        default=config['model']['vae_scale_factor'])
    parser.add_argument("--diffusion-hidden-dim", type=int,
                        default=config['model']['diffusion_hidden_dim'])
    parser.add_argument("--diffusion-num-heads", type=int,
                        default=config['model']['diffusion_num_heads'])
    parser.add_argument("--diffusion-time-emb-dim", type=int,
                        default=config['model']['diffusion_time_emb_dim'])
    # latent_channels is usually fixed by VAE

    # --- Scheduler ---
    parser.add_argument("--train-timesteps", type=int,
                        default=config['scheduler']['train_timesteps'])
    parser.add_argument("--beta-start", type=float,
                        default=config['scheduler']['beta_start'])
    parser.add_argument("--beta-end", type=float,
                        default=config['scheduler']['beta_end'])

    # --- Training ---
    parser.add_argument("--device", type=str,
                        default=config['training']['device'])
    parser.add_argument("--seed", type=int, default=config['training']['seed'])
    parser.add_argument("--num-epochs", type=int,
                        default=config['training']['num_epochs'])
    parser.add_argument("--batch-size", type=int,
                        default=config['training']['batch_size'])
    parser.add_argument("--learning-rate", type=float,
                        default=config['training']['learning_rate'])
    parser.add_argument("--optimizer-weight-decay", type=float,
                        default=config['training']['optimizer_weight_decay'])
    parser.add_argument("--lr-scheduler-type", type=str,
                        default=config['training']['lr_scheduler_type'])
    parser.add_argument("--lr-scheduler-t-max-epochs-factor", type=float,
                        default=config['training']['lr_scheduler_t_max_epochs_factor'])
    parser.add_argument("--lr-scheduler-eta-min", type=float,
                        default=config['training']['lr_scheduler_eta_min'])
    parser.add_argument("--use-amp", type=lambda x: (str(x).lower()
                        == 'true'), default=config['training']['use_amp'])
    parser.add_argument("--grad-clip-max-norm", type=float,
                        default=config['training']['grad_clip_max_norm'], help="Set to 0 or null in YAML to disable")
    parser.add_argument("--empty-prompt-prob", type=float,
                        default=config['training']['empty_prompt_prob'])

    # --- Output ---
    parser.add_argument("--output-dir", type=str,
                        default=config['output']['output_dir'])
    parser.add_argument("--model-subdir", type=str,
                        default=config['output']['model_subdir'])
    parser.add_argument("--plot-subdir", type=str,
                        default=config['output']['plot_subdir'])
    parser.add_argument("--model-filename", type=str,
                        default=config['output']['model_filename'])
    parser.add_argument("--loss-plot-filename", type=str,
                        default=config['output']['loss_plot_filename'])
    parser.add_argument("--save-epoch-freq", type=int,
                        default=config['output']['save_epoch_freq'])
    parser.add_argument("--log-freq", type=int, default=config['output'].get(
        # Use .get for optional keys
        'log_freq', None), help="Log every N steps (optional)")

    # --- Dataloader ---
    parser.add_argument("--num-workers", type=int,
                        default=config['dataloader']['num_workers'])
    parser.add_argument("--pin-memory", type=lambda x: (str(x).lower()
                        == 'true'), default=config['dataloader']['pin_memory'])

    # Parse all arguments including overrides
    args = parser.parse_args()

    # Call main function with parsed arguments
    main(args)
